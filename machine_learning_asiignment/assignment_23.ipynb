{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
    "disadvantages?\n",
    "<br>\n",
    "Ans - >\n",
    "In machine learning classification problems, there are often too many factors on the basis of which the final classification is done. These factors are basically variables called features. The higher the number of features, the harder it gets to visualize the training set and then work on it. Sometimes, most of these features are correlated, and hence redundant. This is where dimensionality reduction algorithms come into play. Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. Basically, it may lead to some amount of data loss. Although, PCA(Principal Component Analysis) tends to find linear correlations between variables, which is sometimes undesirable. Also, PCA fails in cases where mean and covariance are not enough to define datasets. Further, we may not know how many principal components to keep- in practice, some thumb rules are applied.\n",
    "\n",
    "2. What is the dimensionality curse?\n",
    "<br>\n",
    "Ans - >\n",
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions. A higher number of dimensions theoretically allow more information to be stored, but practically it rarely helps due to the higher possibility of noise and redundancy in the real-world data.It will also lead to take a time while creating the model.\n",
    "\n",
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
    "can you go about doing it? If not, what is the reason?\n",
    "<br>\n",
    "Ans - >\n",
    "Once a dataset's dimensionality has been reduced using one of the algorithms we discussed, it is almost always impossible to perfectly reverse the operation, because some information gets lost during dimensionality reduction.Moreover, while some algorithms (such as PCA) have a simple reverse transformation procedure that can reconstruct a dataset relatively similar to the original, other algorithms (such as T-SNE) do not Dimensionality reduction (compression of information) is reversible in auto-encoders. Auto-encoder is regular neural network with bottleneck layer in the middle. You have for instance 20 inputs in the first layer, 10 neurons in the middle layer and again 20 neurons in the last layer. When you train such network you force it to compress information to 10 neurons and then uncompress again minimizing error in the last layer(desired output vector equals input vector). When you use well known Back-propagation algorithm to train such network it performs PCA - Principal Component Analysis. PCA returns uncorrelated features. By using more sophisticated algorithm to train auto-encoder you can make it perform nonlinear ICA - Independent Component Analysis. ICA returns statistically independent features. This training algorithm searches for low complexity neural networks with high generalization capability. As a byproduct of regularization you get ICA.\n",
    "\n",
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "<br>\n",
    "Ans - >\n",
    "PCA can at least get rid of useless dimensions but data should be continous dataset, which will significantly reduce the demensionality of most datasets. However, reducing demensionality of datasets without useless dimensions will loose too much info. \"You want to unroll the swiss roll, not squash it\" OF course, you can still do a PCA computation on nonlinear data - but the results will be meaningless, beyond decomposing to the dominant linear modes and provided a global linear representation of the spread of the data.\n",
    "\n",
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "<br>\n",
    "Ans - >\n",
    "Vanilla: works only if the dataset fits in memory\n",
    "Incremental: useful for large datasets that don't fit in memory, but is slower than regular. Also useful for online tasks, when you apply PCA on the fly\n",
    "Randomized: useful when you want to considerably reduce dimensionality and the dataset fits in memory\n",
    "Kernel: Useful for nonlinear datasets\n",
    "\n",
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset?\n",
    "<br>\n",
    "Ans - >\n",
    "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset.\n",
    "\n",
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "<br>\n",
    "Ans - >\n",
    "You can chain two different algorithms, such as PCA to quickly get rid of unnecessary data and then LLE(Locall Linear Embedded) that works slower. This will likely yield the same performance, but with faster results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
